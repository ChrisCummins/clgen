# A tiny corpus of OpenCL kernels and a correspondingly small model.
# It should take around 30 minutes to train on a reasonably powerful GPU,
# and maybe around an hour on a CPU.
# File: //deeplearning/deepsmith/proto/clgen.proto
# Proto: clgen.Instance
working_dir: "/var/phd/clgen/tiny"
model {
  corpus {
    local_tar_archive: "$PWD/corpus.tar.bz2"
    ascii_character_atomizer: true
    contentfile_separator: "\n\n"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangPreprocessWithShim"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:NormalizeIdentifiers"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:StripDoubleUnderscorePrefixes"
    preprocessor: "deeplearning.clgen.preprocessors.common:StripDuplicateEmptyLines"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:SanitizeKernelPrototype"
    preprocessor: "deeplearning.clgen.preprocessors.common:StripTrailingWhitespace"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:ClangFormat"
    preprocessor: "deeplearning.clgen.preprocessors.common:MinimumLineCount3"
    preprocessor: "deeplearning.clgen.preprocessors.opencl:Compile"
  }
  architecture {
    neuron_type: LSTM
    neurons_per_layer: 256
    num_layers: 2
    post_layer_dropout_micros: 2000  # = 0.2 real value
  }
  training {
    num_epochs: 30
    sequence_length: 50
    batch_size: 64
    shuffle_corpus_contentfiles_between_epochs: true
    rmsprop_optimizer {
      initial_learning_rate_micros: 1000  # = 0.01 real value
      learning_rate_decay_per_epoch_micros: 0  # = 0.0 real value
    }
  }
}
sampler {
  start_text: "kernel void A(global int* a, const int b) {\n  "
  batch_size: 64
  temperature_micros: 1000000
  termination_criteria {
    symtok {
      depth_increase_token: "{"
      depth_decrease_token: "}"
    }
  }
  termination_criteria {
    maxlen {
      maximum_tokens_in_sample: 500
    }
  }
}
